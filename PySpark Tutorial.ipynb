{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful resources:\n",
    "\n",
    "https://www.youtube.com/playlist?list=PL0hSJrxggIQr6wA8buIn1Yxu810ugGed-\n",
    "\n",
    "https://www.tutorialspoint.com/apache_spark/apache_spark_quick_guide.htm\n",
    "\n",
    "https://spark.apache.org/docs/2.2.0/rdd-programming-guide.html\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initializing Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(master = 'local[2]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.86.20:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=pyspark-shell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.version #Retrieve SparkContext version\n",
    "sc.pythonVer #Retrieve Python version\n",
    "sc.master #Master URL to connect to\n",
    "str(sc.sparkHome) #Path where Spark is installed on worker nodes\n",
    "str(sc.sparkUser()) #Retrieve name of the Spark User running SparkContext\n",
    "sc.appName #Return application name\n",
    "sc.applicationId #Retrieve application ID\n",
    "sc.defaultParallelism# Return default level of parallelism\n",
    "sc.defaultMinPartitions #Default minimum number of partitions for RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "conf = (SparkConf().setMaster(\"local\").setAppName(\"My app\").set(\"spark.executor.memory\", \"1g\"))\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.86.20:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>My app</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=My app>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelized Collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([('a',7),('a',2),('b',2)])\n",
    "rdd2 = sc.parallelize([('a',2),('d',1),('b',1)])\n",
    "rdd3 = sc.parallelize(range(100))\n",
    "rdd4 = sc.parallelize([(\"a\",[\"x\",\"y\",\"z\"]),(\"b\",[\"p\", \"r\"])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External Data\n",
    "\n",
    "Read either one text file from HDFS, a local file system, or anyHadoop-supported file system URI with `textFile()`, or read in a directory of text files with `wholeTextFiles()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textFile = sc.textFile(\"/my/directory/*.txt\")\n",
    "textFile2 = sc.wholeTextFiles(\"/my/directory/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Retrieving RDD Information "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.getNumPartitions() #List the number of partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count() #Count RDD instances 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'a': 2, 'b': 1})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.countByKey() #Count RDD instances by key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {('a', 7): 1, ('a', 2): 1, ('b', 2): 1})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.countByValue() #Count RDD instances by value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 2, 'b': 2}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collectAsMap() #Return (key,value) pairs as a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4950"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.sum() #Sum of RDD elements 4950"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([]).isEmpty() #Check whether RDD is empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3.max() #Maximum value of RDD elements 99\n",
    "rdd3.min() #Minimum value of RDD elements 0\n",
    "rdd3.mean() #Mean value of RDD elements 49.5\n",
    "rdd3.stdev() #Standard deviation of RDD elements 28.866070047722118\n",
    "rdd3.variance() #Compute variance of RDD elements 833.25\n",
    "rdd3.histogram(3)# Compute histogram by bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(count: 100, mean: 49.5, stdev: 28.86607004772212, max: 99.0, min: 0.0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.stats() #Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Applying Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 7, 7, 'a'), ('a', 2, 2, 'a'), ('b', 2, 2, 'b')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda x: x+(x[1],x[0])).collect() #Apply a function to each RDD element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 7, 7, 'a', 'a', 2, 2, 'a', 'b', 2, 2, 'b']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd5 = rdd.flatMap(lambda x: x+(x[1],x[0])) #Apply a function to each RDD element and flatten the result\n",
    "rdd5.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd4.flatMapValues(lambda x: x).collect() #Apply a flatMap function to each (key,value) pair of rdd4 without changing the keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Selecting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 7), ('a', 2), ('b', 2)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect() #Return a list with all RDD elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 7), ('a', 2)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(2) #Take first 2 RDD elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a', 7)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.first() #Take first RDD element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 2), ('a', 7)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.top(2) #Take top 2 RDD elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 27, 28, 35, 41, 43, 49, 53, 58, 85, 93]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.sample(False, 0.15, 81).collect() #Return sampled subset of rdd3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 7), ('a', 2)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(lambda x: \"a\" in x).collect() # Filter the RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 7, 2, 'b']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd5.distinct().collect() #Return distinct RDD values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'a', 'b']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.keys().collect() #Return (key,value) RDD's keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Reshaping Data\n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 9), ('b', 2)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.reduceByKey(lambda x,y : x+y).collect() #Merge the rdd values for each key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a', 7, 'a', 2, 'b', 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.reduce(lambda a, b: a + b) #Merge the rdd values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  [0,\n",
       "   2,\n",
       "   4,\n",
       "   6,\n",
       "   8,\n",
       "   10,\n",
       "   12,\n",
       "   14,\n",
       "   16,\n",
       "   18,\n",
       "   20,\n",
       "   22,\n",
       "   24,\n",
       "   26,\n",
       "   28,\n",
       "   30,\n",
       "   32,\n",
       "   34,\n",
       "   36,\n",
       "   38,\n",
       "   40,\n",
       "   42,\n",
       "   44,\n",
       "   46,\n",
       "   48,\n",
       "   50,\n",
       "   52,\n",
       "   54,\n",
       "   56,\n",
       "   58,\n",
       "   60,\n",
       "   62,\n",
       "   64,\n",
       "   66,\n",
       "   68,\n",
       "   70,\n",
       "   72,\n",
       "   74,\n",
       "   76,\n",
       "   78,\n",
       "   80,\n",
       "   82,\n",
       "   84,\n",
       "   86,\n",
       "   88,\n",
       "   90,\n",
       "   92,\n",
       "   94,\n",
       "   96,\n",
       "   98]),\n",
       " (1,\n",
       "  [1,\n",
       "   3,\n",
       "   5,\n",
       "   7,\n",
       "   9,\n",
       "   11,\n",
       "   13,\n",
       "   15,\n",
       "   17,\n",
       "   19,\n",
       "   21,\n",
       "   23,\n",
       "   25,\n",
       "   27,\n",
       "   29,\n",
       "   31,\n",
       "   33,\n",
       "   35,\n",
       "   37,\n",
       "   39,\n",
       "   41,\n",
       "   43,\n",
       "   45,\n",
       "   47,\n",
       "   49,\n",
       "   51,\n",
       "   53,\n",
       "   55,\n",
       "   57,\n",
       "   59,\n",
       "   61,\n",
       "   63,\n",
       "   65,\n",
       "   67,\n",
       "   69,\n",
       "   71,\n",
       "   73,\n",
       "   75,\n",
       "   77,\n",
       "   79,\n",
       "   81,\n",
       "   83,\n",
       "   85,\n",
       "   87,\n",
       "   89,\n",
       "   91,\n",
       "   93,\n",
       "   95,\n",
       "   97,\n",
       "   99])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.groupBy(lambda x: x % 2)\\\n",
    " .mapValues(list)\\\n",
    " .collect()#Return RDD of grouped values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', [7, 2]), ('b', [2])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.groupByKey().mapValues(list).collect() #Group rdd by key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`seqFunc` runs firstly and it makes the given calculation with zeroValue and current value and then create an output value. `combFunc` uses this output value and make the given calculation with current and previous output value of seqFunc function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4950, 100)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqOp = (lambda x,y: (x[0]+y,x[1]+1))\n",
    "combOp = (lambda x,y:(x[0]+y[0],x[1]+y[1]))\n",
    "rdd3.aggregate((0,0),seqOp,combOp) #Aggregate RDD elements of each partition and then the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (9, 2)), ('b', (2, 1))]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.aggregateByKey((0,0),seqOp,combOp).collect() #Aggregate values of each RDD key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4950"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "rdd3.fold(0,add) #Aggregate the elements of each partition, and then the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 9), ('b', 2)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.foldByKey(0, add).collect() #Merge the values for each key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (2, 1),\n",
       " (4, 2),\n",
       " (6, 3),\n",
       " (8, 4),\n",
       " (10, 5),\n",
       " (12, 6),\n",
       " (14, 7),\n",
       " (16, 8),\n",
       " (18, 9),\n",
       " (20, 10),\n",
       " (22, 11),\n",
       " (24, 12),\n",
       " (26, 13),\n",
       " (28, 14),\n",
       " (30, 15),\n",
       " (32, 16),\n",
       " (34, 17),\n",
       " (36, 18),\n",
       " (38, 19),\n",
       " (40, 20),\n",
       " (42, 21),\n",
       " (44, 22),\n",
       " (46, 23),\n",
       " (48, 24),\n",
       " (50, 25),\n",
       " (52, 26),\n",
       " (54, 27),\n",
       " (56, 28),\n",
       " (58, 29),\n",
       " (60, 30),\n",
       " (62, 31),\n",
       " (64, 32),\n",
       " (66, 33),\n",
       " (68, 34),\n",
       " (70, 35),\n",
       " (72, 36),\n",
       " (74, 37),\n",
       " (76, 38),\n",
       " (78, 39),\n",
       " (80, 40),\n",
       " (82, 41),\n",
       " (84, 42),\n",
       " (86, 43),\n",
       " (88, 44),\n",
       " (90, 45),\n",
       " (92, 46),\n",
       " (94, 47),\n",
       " (96, 48),\n",
       " (98, 49),\n",
       " (100, 50),\n",
       " (102, 51),\n",
       " (104, 52),\n",
       " (106, 53),\n",
       " (108, 54),\n",
       " (110, 55),\n",
       " (112, 56),\n",
       " (114, 57),\n",
       " (116, 58),\n",
       " (118, 59),\n",
       " (120, 60),\n",
       " (122, 61),\n",
       " (124, 62),\n",
       " (126, 63),\n",
       " (128, 64),\n",
       " (130, 65),\n",
       " (132, 66),\n",
       " (134, 67),\n",
       " (136, 68),\n",
       " (138, 69),\n",
       " (140, 70),\n",
       " (142, 71),\n",
       " (144, 72),\n",
       " (146, 73),\n",
       " (148, 74),\n",
       " (150, 75),\n",
       " (152, 76),\n",
       " (154, 77),\n",
       " (156, 78),\n",
       " (158, 79),\n",
       " (160, 80),\n",
       " (162, 81),\n",
       " (164, 82),\n",
       " (166, 83),\n",
       " (168, 84),\n",
       " (170, 85),\n",
       " (172, 86),\n",
       " (174, 87),\n",
       " (176, 88),\n",
       " (178, 89),\n",
       " (180, 90),\n",
       " (182, 91),\n",
       " (184, 92),\n",
       " (186, 93),\n",
       " (188, 94),\n",
       " (190, 95),\n",
       " (192, 96),\n",
       " (194, 97),\n",
       " (196, 98),\n",
       " (198, 99)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.keyBy(lambda x: x+x).collect() #Create tuples of RDD elements by applying a function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Mathematical Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 7), ('b', 2)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.subtract(rdd2).collect() #Return each rdd value not contained in rdd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('d', 1)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.subtractByKey(rdd).collect() #Return each (key,value) pair of rdd2 with no matching key in rdd [('d', 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('a', 7), ('a', 2)),\n",
       " (('a', 7), ('d', 1)),\n",
       " (('a', 7), ('b', 1)),\n",
       " (('a', 2), ('a', 2)),\n",
       " (('a', 2), ('d', 1)),\n",
       " (('a', 2), ('b', 1)),\n",
       " (('b', 2), ('a', 2)),\n",
       " (('b', 2), ('d', 1)),\n",
       " (('b', 2), ('b', 1))]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.cartesian(rdd2).collect() #Return the Cartesian product of rdd and rdd2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x):\n",
    "    print(x)\n",
    "    \n",
    "rdd.foreach(g) #Apply a function to all RDD elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('d', 1), ('b', 1), ('a', 2)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.sortBy(lambda x: x[1]).collect() #Sort RDD by given function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 2), ('b', 1), ('d', 1)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.sortByKey().collect() #Sort (key, value) RDD by key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repartitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.repartition(4) #New RDD with 4 partitions\n",
    "rdd.coalesce(1) #Decrease the number of partitions in the RDD to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.saveAsTextFile(\"rdd.txt\")\n",
    "\n",
    "rdd.saveAsHadoopFile(\"hdfs://namenodehost/parent/child\", 'org.apache.hadoop.mapred.TextOutputFormat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stopping SparkContext\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execution\n",
    "'''\n",
    "$ ./bin/spark-submit examples/src/main/python/pi.py\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame & SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initializing SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A SparkSession can be used create DataFrame, register DataFrame as tables, execute SQL over tables, cache tables, and read parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    " .builder \\\n",
    " .appName(\"Python Spark SQL basic example\") \\\n",
    " .config(\"spark.some.config.option\", \"some-value\") \\\n",
    " .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.86.20:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Python Spark SQL basic example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1e079e52f88>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# Infer Schema\n",
    "sc = spark.sparkContext\n",
    "lines = sc.textFile(\"people.txt\")\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "people = parts.map(lambda p: Row(name=p[0],age=int(p[1])))\n",
    "peopledf = spark.createDataFrame(people)\n",
    "\n",
    "# Specify Schema\n",
    "people = parts.map(lambda p: Row(name=p[0],age=int(p[1].strip())))\n",
    "schemaString = \"name age\"\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "schema = StructType(fields)\n",
    "spark.createDataFrame(people, schema).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Spark Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON\n",
    "df = spark.read.json(\"customer.json\")\n",
    "df.show()\n",
    " +--------------------+---+---------+--------+--------------------+\n",
    " | address|age|firstName |lastName| phoneNumber|\n",
    " +--------------------+---+---------+--------+--------------------+\n",
    " |[New York,10021,N...| 25| John| Smith|[[212 555-1234,ho...|\n",
    " |[New York,10021,N...| 21| Jane| Doe|[[322 888-1234,ho...|\n",
    " +--------------------+---+---------+--------+--------------------+\n",
    "df2 = spark.read.load(\"people.json\", format=\"json\")\n",
    "                                        \n",
    "# Parquet files\n",
    "df3 = spark.read.load(\"users.parquet\")\n",
    "# TXT files\n",
    "df4 = spark.read.text(\"people.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes #Return df column names and data types\n",
    "df.show() #Display the content of df\n",
    "df.head() #Return first n rows\n",
    "df.first() #Return first row\n",
    "df.take(2) #Return the first n rows >>> df.schema Return the schema of df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().show()# Compute summary statistics\n",
    "df.columns #Return the columns of df\n",
    "df.count() #Count the number of rows in df\n",
    "df.distinct().count() #Count the number of distinct rows in df\n",
    "df.printSchema() #Print the schema of df\n",
    "df.explain() #Print the (logical and physical) plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropDuplicates() #Drop duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"firstName\").show() #Show all entries in firstName column\n",
    "\n",
    "df.select(\"firstName\",\"lastName\") \\\n",
    " .show()\n",
    "\n",
    "df.select(\"firstName\", \"age\",explode(\"phoneNumber\").alias(\"contactInfo\"))\\\n",
    " .select(\"contactInfo.type\", \"firstName\", \"age\") \\\n",
    " .show() # Show all entries in firstName, age and type\n",
    "\n",
    "\n",
    "df.select(df[\"firstName\"],df[\"age\"]+ 1).show() #Show all entries in firstName and age, add 1 to the entries of age\n",
    "\n",
    "\n",
    "df.select(df['age'] > 24).show() #Show all entries where age >24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"firstName\", F.when(df.age > 30, 1).otherwise(0)).show() #Show firstName and 0 or 1 depending on age >30\n",
    "\n",
    "df[df.firstName.isin(\"Jane\",\"Boris\")].collect() #Show firstName if in the given options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"firstName\", df.lastName.like(\"Smith\").show() #Show firstName, and lastName is TRUE if lastName is like Smith"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Startswith - Endswith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"firstName\",df.lastName.startswith(\"Sm\")).show() #Show firstName, and TRUE if lastName starts with Sm\n",
    "\n",
    "df.select(df.lastName.endswith(\"th\")).show() # Show last names ending in th"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Substring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(df.firstName.substr(1, 3).alias(\"name\")).collect() #Return substrings of firstName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(df.age.between(22, 24)).show() #Show age: values are TRUE if between  22 and 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Add, Update & Remove Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add\n",
    "df = df.withColumn('city',df.address.city) \\\n",
    " .withColumn('postalCode',df.address.postalCode) \\\n",
    " .withColumn('state',df.address.state) \\\n",
    " .withColumn('streetAddress',df.address.streetAddress) \\\n",
    " .withColumn('telePhoneNumber', explode(df.phoneNumber.number)) \\\n",
    " .withColumn('telePhoneType', explode(df.phoneNumber.type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update\n",
    "df = df.withColumnRenamed('telePhoneNumber', 'phoneNumber')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove\n",
    "df = df.drop(\"address\", \"phoneNumber\")\n",
    "df = df.drop(df.address).drop(df.phoneNumber)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"age\".count().show() #Group by age, count the members n the groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df[\"age\"]>24).show() #Filter entries of age, only keep those records of which the values are >24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopledf.sort(peopledf.age.desc()).collect()\n",
    "\n",
    "df.sort(\"age\", ascending=False).collect()\n",
    "\n",
    "df.orderBy([\"age\",\"city\"],ascending=[0,1]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing & Replacing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.na.fill(50).show() #Replace null values\n",
    "\n",
    "df.na.drop().show() #Return new df omitting rows with null values\n",
    "\n",
    "df.na.replace(10, 20).show() \\ #Return new df replacing one value with another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repartitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.repartition(10).rdd.getNumPartitions() # df with 10 partitions\n",
    "\n",
    "df.coalesce(1).rdd.getNumPartitions() #df with 1 partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Running SQL Queries Programmatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registering DataFrames as Views\n",
    "peopledf.createGlobalTempView(\"people\")\n",
    "\n",
    "df.createTempView(\"customer\")\n",
    "\n",
    "df.createOrReplaceTempView(\"customer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Views\n",
    "df5 = spark.sql(\"SELECT * FROM customer\").show()\n",
    "peopledf2 = spark.sql(\"SELECT * FROM global_temp.people\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Structures\n",
    "\n",
    "rdd1 = df.rdd #Convert df into an RDD\n",
    "\n",
    "df.toJSON().first() #Convert df into a RDD of string\n",
    "\n",
    "df.toPandas()# Return the contents of df as Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write & Save to Files\n",
    "df.select(\"firstName\", \"city\")\\\n",
    " .write \\\n",
    " .save(\"nameAndCity.parquet\")\n",
    "\n",
    "df.select(\"firstName\", \"age\") \\\n",
    " .write \\\n",
    " .save(\"namesAndAges.json\",format=\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopping SparkSession\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
